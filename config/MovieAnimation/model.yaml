transformer:
  encoder_layer: 4
  encoder_head: 2
  encoder_hidden: 256
  decoder_layer: 6
  decoder_head: 2
  decoder_hidden: 256
  conv_filter_size: 1024
  conv_kernel_size: [9, 1]
  encoder_dropout: 0.2
  decoder_dropout: 0.2

variance_predictor:
  filter_size: 256
  kernel_size: 3
  dropout: 0.5

variance_embedding:
  pitch_quantization: "linear" # support 'linear' or 'log', 'log' is allowed only if the pitch values are not normalized during preprocessing
  energy_quantization: "linear" # support 'linear' or 'log', 'log' is allowed only if the energy values are not normalized during preprocessing
  n_bins: 256

# gst:
#   use_gst: False
#   conv_filters: [32, 32, 64, 64, 128, 128]
#   gru_hidden: 128
#   token_size: 128
#   n_style_token: 10
#   attn_head: 4

multi_speaker: True
with_emotion: True

learn_speaker: True # whether use Embedding layer to learn speaker embedding
learn_emotion: False # whether use Embedding layer to learn emotion embedding

max_seq_len: 1000

vocoder:
  model: "HiFi-GAN" # support 'HiFi-GAN', 'MelGAN'
  speaker: "universal" # support  'LJSpeech', 'universal'

Multimodal-Transformer:
  orig_d_l: 256
  orig_d_a: 256
  orig_d_v: 256
  attn_dropout: 0.1
  attn_dropout_a: 0.0
  attn_dropout_v: 0.0
  relu_dropout: 0.1
  embed_dropout: 0.25
  res_dropout: 0.1
  out_dropout: 0.0
  num_heads: 8
  attn_mask: True
  layers: 5
